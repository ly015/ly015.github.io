# Publications

<!-- MotionBooth -->
<div class='paper-box' id="pub-motionbooth"><div class='paper-box-image'>
<div><div class="badge">NeurIPS 2024</div><img src='images/motionbooth.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MotionBooth: Motion-Aware Customized Text-to-Video Generation](https://arxiv.org/abs/2406.17758)

Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, Kai Chen

***NeurIPS, 2024 (Spotlight)***<br>
[Project](https://jianzongwu.github.io/projects/motionbooth/) $$|$$ [Code](https://github.com/jianzongwu/MotionBooth) $$|$$ [Demo](https://www.youtube.com/watch?v=iuH5iqLk5VQ)

</div>
</div>

<!-- ADC -->
<div class='paper-box' id='pub-adc'><div class='paper-box-image'>
<div><div class="badge">NeurIPS 2024</div><img src='images/adc_llm_jailbreak.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Efficient LLM Jailbreak via Adaptive Dense-to-sparse Constrained Optimization](https://arxiv.org/abs/2405.09113)

Kai Hu, Weichen Yu, Tianjun Yao, Xiang Li, Wenhe Liu, Lijun Yu, Yining Li, Kai Chen, Zhiqiang Shen, Matt Fredrikson

***NeurIPS, 2024***<br>
[Arxiv](https://arxiv.org/abs/2405.09113)

</div>
</div>

<!-- XComposer2-4kHD -->
<div class='paper-box' id='pub-xcomposer2-4khd'><div class='paper-box-image'>
<div><div class="badge">NeurIPS</div><img src='images/xcomposer2-4khd.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Internlm-xcomposer2-4khd: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd](https://arxiv.org/pdf/2404.06512)

<details>
    <summary> Full author list </summary>
    <p>Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Zhe Chen, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Kai Chen, Conghui He, Xingcheng Zhang, Jifeng Dai, Yu Qiao, Dahua Lin, Jiaqi Wang</p>
</details>

***NeurIPS, 2024***<br>
[Project](https://github.com/InternLM/InternLM-XComposer) $$|$$ [HuggingFace](https://huggingface.co/internlm/internlm-xcomposer2-4khd-7b) $$|$$ [ArXiv](https://arxiv.org/pdf/2404.06512)

</div>
</div>

<!-- InternLM2 -->
<div class='paper-box' id="#pub-internlm2"><div class='paper-box-image'>
<div><div class="badge">Technical Report</div><img src='images/internlm.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[InternLM2 technical report](https://arxiv.org/pdf/2403.17297)

<details>
    <summary> Full author list </summary>
    <p>Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, Dahua Lin</p>
</details>

***Arxiv, 2024***<br>
[Project](https://github.com/InternLM/InternLM) $$|$$ [HuggingFace](https://huggingface.co/internlm) $$|$$ [ArXiv](https://arxiv.org/pdf/2403.17297)

</div>
</div>

<!-- XComposer -->
<div class='paper-box'><div class='paper-box-image'>
<div><div class="badge">Technical Report</div><img src='images/xcomposer.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model](https://arxiv.org/pdf/2401.16420)

<details>
    <summary> Full author list </summary>
    <p>Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, Jiaqi Wang</p>
</details>

***Arxiv, 2024***<br>
[Project](https://github.com/InternLM/InternLM-XComposer) $$|$$ [HuggingFace](https://huggingface.co/collections/internlm/internlm-xcomposer2-65b3706bf5d76208998e7477) $$|$$ [ArXiv](https://arxiv.org/pdf/2401.16420)

</div>
</div>

<!-- GTA -->
<div class='paper-box' id='pub-gta'><div class='paper-box-image'>
<div><div class="badge">NeurIPS</div><img src='images/GTA.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[GTA: A Benchmark for General Tool Agents](https://arxiv.org/pdf/2407.08713)

Jize Wang, Zerun Ma, Yining Li, Songyang Zhang, Cailian Chen, Kai Chen, Xinyi Le

***NeurIPS, 2024 (Track Datasets and Benchmarks)***<br>
[Project](https://open-compass.github.io/GTA/) $$|$$ [Code](https://github.com/open-compass/GTA) $$|$$ [ArXiv](https://arxiv.org/pdf/2407.08713) $$|$$ [Dataset](https://github.com/open-compass/GTA/releases/download/v0.1.0/gta_dataset.zip)

</div>
</div>

<!-- RTMW -->
<div class='paper-box' id='pub-rtmw'><div class='paper-box-image'>
<div><div class="badge">CVPR</div><img src='images/rtmw.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[RTMW: Real-Time Multi-Person 2D and 3D Whole-body Pose Estimation](https://arxiv.org/pdf/2407.08634)

Tao Jiang$$^*$$, Xinchen Xie$$^*$$, Yining Li

***CVPR, 2024***<br>
[Code](https://github.com/open-mmlab/mmpose/tree/main/projects/rtmpose3d) $$|$$ [ArXiv](https://arxiv.org/pdf/2407.08634)

</div>
</div>

<!-- OVSAM -->
<div class='paper-box' id="#pub-ovsam"><div class='paper-box-image'>
<div><div class="badge">ECCV</div><img src='images/ovsam.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Open-vocabulary SAM: Segment and recognize twenty-thousand classes interactively](https://arxiv.org/abs/2401.02955)

Haobo Yuan, Xiangtai Li, Chong Zhou, Yining Li, Kai Chen, Chen Change Loy

***ECCV, 2024***<br>
[Project](https://www.mmlab-ntu.com/project/ovsam/) $$|$$ [Code](https://github.com/HarborYuan/ovsam) $$|$$ [ArXiv](https://arxiv.org/abs/2401.02955) $$|$$ [Demo](https://openxlab.org.cn/apps/detail/houshaowei/Open-Vocabulary_SAM)

</div>
</div>

<!-- RTMO -->
<div class='paper-box' id="#rtmo"><div class='paper-box-image'>
<div><div class="badge">Arxiv</div><img src='images/rtmo.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[RTMO: Towards High-Performance One-Stage Real-Time Multi-Person Pose Estimation](https://arxiv.org/abs/2312.07526)

Peng Lu, Tao Jiang, Yining Li, Xiangtai Li, Kai Chen, Wenming Yang

***CVPR, 2024***<br>
[Code](https://github.com/open-mmlab/mmpose/tree/main/projects/rtmo) $$|$$ [ArXiv](https://arxiv.org/abs/2312.07526) $$|$$ [Demo](https://openxlab.org.cn/apps/detail/mmpose/RTMPose)

</div>
</div>

<!-- OMG-Seg -->
<div class='paper-box' id="#pub-omgseg"><div class='paper-box-image'>
<div><div class="badge">CVPR</div><img src='images/omg_seg.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[OMG-Seg: Is one model good enough for all segmentation?](https://arxiv.org/abs/2401.10229)

Xiangtai Li, Haobo Yuan, Wei Li, Henghui Ding, Size Wu, Wenwei Zhang, Yining Li, Kai Chen, Chen Change Loy

***CVPR, 2024***<br>
[Project](https://lxtgh.github.io/project/omg_seg/) $$|$$ [Code](https://github.com/lxtGH/OMG-Seg) $$|$$ [ArXiv](https://arxiv.org/abs/2401.10229)

</div>
</div>

<!-- ROVI -->
<div class='paper-box' id="#pub-rovi"><div class='paper-box-image'>
<div><div class="badge">CVPR</div><img src='images/rovi.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Towards language-driven video inpainting via multimodal large language models](https://arxiv.org/abs/2401.10226)

Jianzong Wu, Xiangtai Li, Chenyang Si, Shangchen Zhou, Jingkang Yang, Jiangning Zhang, Yining Li, Kai Chen, Yunhai Tong, Ziwei Liu, Chen Change Loy

***CVPR, 2024***<br>
[Project](https://jianzongwu.github.io/projects/rovi/) $$|$$ [Code](https://github.com/jianzongwu/Language-Driven-Video-Inpainting) $$|$$ [ArXiv](https://arxiv.org/abs/2401.10226)

</div>
</div>


<!-- RTMPose -->
<div class='paper-box'><div class='paper-box-image'>
<div><div class="badge">Arxiv</div><img src='images/rtmpose.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Rtmpose: Real-time multi-person pose estimation based on mmpose](https://arxiv.org/abs/2303.07399)

Tao Jiang, Peng Lu, Li Zhang, Ningsheng Ma, Rui Han, Chengqi Lyu, Yining Li, Kai Chen

***ArXiv, 2023***<br>
[Code](https://github.com/open-mmlab/mmpose/tree/main/projects/rtmpose) $$|$$ [ArXiv](https://arxiv.org/abs/2401.10226) $$|$$ [Demo](https://openxlab.org.cn/apps/detail/mmpose/RTMPose)

</div>
</div>

<!-- deep-imbalanced-learning -->
<div class='paper-box'><div class='paper-box-image'>
<div><div class="badge">T-PAMI</div><img src='images/DIL.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Deep imbalanced learning for face recognition and attribute prediction](https://arxiv.org/pdf/1806.00194)

Chen Huang, Yining Li, Chen Change Loy, Xiaoou Tang

***T-PAMI, 2019***<br>
[ArXiv](https://arxiv.org/pdf/1806.00194)

</div>
</div>

<!-- DIAF -->
<div class='paper-box'><div class='paper-box-image'>
<div><div class="badge">CVPR</div><img src='images/diaf_cvpr2019.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Dense intrinsic appearance flow for human pose transfer](https://arxiv.org/abs/1903.11326)

Yining Li, Chen Huang, Chen Change Loy

***CVPR, 2019***<br>
[Project](https://mmlab.ie.cuhk.edu.hk/projects/pose-transfer/) $$|$$ [Code](https://github.com/ly015/intrinsic_flow) $$|$$ [ArXiv](https://arxiv.org/abs/1903.11326)

</div>
</div>

<!-- VDQG -->
<div class='paper-box'><div class='paper-box-image'>
<div><div class="badge">ICCV</div><img src='images/vdqg_iccv2017.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Learning to disambiguate by asking discriminative questions](https://arxiv.org/abs/1708.02760)

Yining Li, Chen Huang, Xiaoou Tang, Chen-Change Loy

***ICCV, 2017***<br>
[Project](http://mmlab.ie.cuhk.edu.hk/projects/vdqg/) $$|$$ [Dataset](http://mmlab.ie.cuhk.edu.hk/projects/vdqg/#dataset) $$|$$ [ArXiv](https://arxiv.org/abs/1708.02760)

</div>
</div>

<!-- LMLE -->
<div class='paper-box'><div class='paper-box-image'>
<div><div class="badge">CVPR</div><img src='images/LMLE_cvpr2016.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Learning Deep Representation for Imbalanced Classification](https://openaccess.thecvf.com/content_cvpr_2016/papers/Huang_Learning_Deep_Representation_CVPR_2016_paper.pdf)

Chen Huang, Yining Li, Chen Change Loy, Xiaoou Tang

***CVPR, 2016 (Spotlight)***<br>
[Project](http://mmlab.ie.cuhk.edu.hk/projects/LMLE.html) $$|$$ [Slides](http://mmlab.ie.cuhk.edu.hk/projects/LMLE/Slides.pdf) $$|$$ [Code](http://mmlab.ie.cuhk.edu.hk/projects/LMLE/lmle_code.zip)

</div>
</div>

<!-- WIDER Attribute -->
<div class='paper-box'><div class='paper-box-image'>
<div><div class="badge">ECCV</div><img src='images/wider_attribute_eccv2016.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Human attribute recognition by deep hierarchical contexts](https://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2016_human.pdf)

Yining Li, Chen Huang, Chen Change Loy, Xiaoou Tang

***ECCV, 2016***<br>
[Project](http://mmlab.ie.cuhk.edu.hk/projects/WIDERAttribute.html) $$|$$ [Dataset](https://drive.google.com/open?id=0B-PXtfvNMLanWEVCaHZnR0RHSlE)

</div>
</div>